#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pexels APIë¡œ Future Tech B-roll ì˜ìƒ ê²€ìƒ‰
"""

import os
import sys
import json
import requests
from openai import OpenAI

def extract_keywords(script):
    """GPTë¡œ ëŒ€ë³¸ì—ì„œ ì˜ìƒ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    print("ğŸ” Extracting visual keywords from script...")
    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    
    prompt = f"""Extract 16 visual search keywords from this Future Tech script for finding B-roll footage.

Script:
{script[:2000]}

Requirements:
- Focus on VISUAL concepts (things you can SEE and FILM)
- Future tech themes: AI, robots, technology, computers, digital, data, space, innovation
- Cinematic and professional
- Mix of: close-ups, wide shots, abstract concepts
- Each keyword 1-3 words
- In English

Examples:
- "artificial intelligence"
- "robotic arm"
- "data center"
- "futuristic city"
- "quantum computer"
- "space station"
- "holographic display"
- "neural network visualization"

Return EXACTLY 16 keywords, one per line, no numbering, no extra text."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a cinematography expert specializing in tech and science footage."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        
        keywords_text = response.choices[0].message.content.strip()
        keywords = [k.strip().strip('"').strip("'").strip('-').strip() 
                   for k in keywords_text.split('\n') if k.strip()]
        
        # ì •í™•íˆ 16ê°œ ë³´ì¥
        keywords = keywords[:16]
        while len(keywords) < 16:
            keywords.append("technology innovation")
        
        print(f"âœ… Extracted {len(keywords)} keywords")
        for i, kw in enumerate(keywords, 1):
            print(f"   {i}. {kw}")
        
        return keywords
        
    except Exception as e:
        print(f"âš ï¸ Keyword extraction failed: {e}")
        # ê¸°ë³¸ Future Tech í‚¤ì›Œë“œ
        return [
            "artificial intelligence", "robot technology", "data center",
            "quantum computer", "futuristic city", "space exploration",
            "holographic display", "neural network", "autonomous vehicle",
            "virtual reality", "biotechnology lab", "smart city",
            "rocket launch", "digital technology", "innovation concept",
            "future technology"
        ]

def search_pexels(keyword, api_key, orientation="landscape", per_page=5):
    """Pexelsì—ì„œ ì˜ìƒ ê²€ìƒ‰"""
    
    url = "https://api.pexels.com/videos/search"
    headers = {"Authorization": api_key}
    params = {
        "query": keyword,
        "orientation": orientation,
        "size": "large",
        "per_page": per_page
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âš ï¸ Pexels search failed for '{keyword}': {e}")
        return {"videos": []}

def search_videos():
    """ë©”ì¸ í•¨ìˆ˜: B-roll ì˜ìƒ ê²€ìƒ‰ ë° ì €ì¥"""
    
    print("ğŸ¬ Searching Future Tech B-roll footage on Pexels...")
    
    # API í‚¤ í™•ì¸
    pexels_api_key = os.environ.get("PEXELS_API_KEY")
    if not pexels_api_key:
        print("âŒ PEXELS_API_KEY not found!")
        sys.exit(1)
    
    # ëŒ€ë³¸ ì½ê¸°
    try:
        with open("temp/script.txt", "r", encoding="utf-8") as f:
            script = f.read()
    except FileNotFoundError:
        print("âŒ Script not found at temp/script.txt")
        sys.exit(1)
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(script)
    
    # ì˜ìƒ ê²€ìƒ‰ (í‚¤ì›Œë“œë‹¹ 1ê°œì”©, ì´ 16ê°œ)
    all_videos = []
    
    for i, keyword in enumerate(keywords, 1):
        print(f"\nğŸ” [{i}/16] Searching: '{keyword}'")
        
        result = search_pexels(keyword, pexels_api_key, per_page=5)
        videos = result.get("videos", [])
        
        if videos:
            # ê°€ì¥ ì¢‹ì€ ì˜ìƒ ì„ íƒ (1080p ì´ìƒ, duration > 5ì´ˆ)
            selected = None
            for video in videos:
                duration = video.get("duration", 0)
                video_files = video.get("video_files", [])
                
                # 1080p íŒŒì¼ ì°¾ê¸°
                hd_files = [vf for vf in video_files 
                           if vf.get("height", 0) >= 1080 and vf.get("width", 0) >= 1920]
                
                if hd_files and duration >= 5:
                    selected = {
                        "id": video.get("id"),
                        "keyword": keyword,
                        "duration": duration,
                        "url": hd_files[0].get("link"),
                        "width": hd_files[0].get("width"),
                        "height": hd_files[0].get("height")
                    }
                    break
            
            if selected:
                all_videos.append(selected)
                print(f"   âœ… Found: {selected['duration']}s, {selected['width']}x{selected['height']}")
            else:
                print(f"   âš ï¸ No suitable video found")
        else:
            print(f"   âš ï¸ No results")
    
    # ê²°ê³¼ í™•ì¸
    if len(all_videos) < 10:
        print(f"\nâš ï¸ Warning: Only {len(all_videos)} videos found (minimum 10 recommended)")
        print("   Filling with backup searches...")
        
        # ë°±ì—… í‚¤ì›Œë“œë¡œ ë¶€ì¡±ë¶„ ì±„ìš°ê¸°
        backup_keywords = [
            "technology abstract", "digital data", "computer network",
            "innovation concept", "futuristic background", "tech visualization"
        ]
        
        for keyword in backup_keywords:
            if len(all_videos) >= 16:
                break
            
            result = search_pexels(keyword, pexels_api_key, per_page=3)
            videos = result.get("videos", [])
            
            for video in videos:
                if len(all_videos) >= 16:
                    break
                
                duration = video.get("duration", 0)
                video_files = video.get("video_files", [])
                hd_files = [vf for vf in video_files if vf.get("height", 0) >= 1080]
                
                if hd_files and duration >= 5:
                    all_videos.append({
                        "id": video.get("id"),
                        "keyword": keyword,
                        "duration": duration,
                        "url": hd_files[0].get("link"),
                        "width": hd_files[0].get("width"),
                        "height": hd_files[0].get("height")
                    })
    
    # ì €ì¥
    with open("temp/videos.json", "w", encoding="utf-8") as f:
        json.dump(all_videos, f, indent=2)
    
    total_duration = sum(v["duration"] for v in all_videos)
    print(f"\nâœ… Search completed!")
    print(f"ğŸ“Š Found {len(all_videos)} videos")
    print(f"â±ï¸ Total duration: {total_duration:.1f} seconds")
    print(f"ğŸ’¾ Saved to: temp/videos.json")
    
    if len(all_videos) < 10:
        print(f"\nâš ï¸ WARNING: Only {len(all_videos)} videos (need 10+ for 6-8 min video)")
        print("   Video creation may fail or be shorter than expected")

if __name__ == "__main__":
    search_videos()
